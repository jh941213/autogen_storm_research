"""AutoGen STORM Research Assistant ë©”ì¸ ì‹¤í–‰ íŒŒì¼

ì´ íŒŒì¼ì€ AutoGen ê¸°ë°˜ì˜ STORM Research Assistantë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import argparse
from dotenv import load_dotenv
from autogen_storm import create_storm_workflow, initialize_tracing, get_tracing_manager
from autogen_storm.config import StormConfig, ModelConfig, ModelProvider
from autogen_storm.models import ResearchTask

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description="STORM Research Assistant")
    parser.add_argument("--topic", type=str, help="ì—°êµ¬ ì£¼ì œ")
    parser.add_argument("--interactive", action="store_true", help="ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (íœ´ë¨¼ í”¼ë“œë°± í¬í•¨)")
    parser.add_argument("--auto", action="store_true", help="ì™„ì „ ìë™ ëª¨ë“œ (íœ´ë¨¼ í”¼ë“œë°± ì—†ìŒ)")
    parser.add_argument("--max-analysts", type=int, default=3, help="ìµœëŒ€ ë¶„ì„ê°€ ìˆ˜")
    parser.add_argument("--max-interview-turns", type=int, default=3, help="ìµœëŒ€ ì¸í„°ë·° í„´ ìˆ˜")
    parser.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ ì¸í„°ë·° í™œì„±í™”")
    
    args = parser.parse_args()
    
    print("ğŸŒŸ AutoGen STORM Research Assistant")
    print("=" * 50)
    
    # Langfuse ì¶”ì  ì´ˆê¸°í™”
    print("\nğŸ” Langfuse ì¶”ì  ì´ˆê¸°í™” ì¤‘...")
    tracing_enabled = initialize_tracing()
    if tracing_enabled:
        print("âœ… Langfuse ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸  Langfuse ì¶”ì ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (API í‚¤ í™•ì¸ í•„ìš”)")
    
    # ëª¨ë¸ ì œê³µì ì„ íƒ
    if args.auto:
        # ìë™ ëª¨ë“œì—ì„œëŠ” Azure OpenAIë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
        choice = "2"
        print("\nğŸ¤– ìë™ ëª¨ë“œ: Azure OpenAI ëª¨ë¸ ì‚¬ìš©")
    else:
        print("\nğŸ¤– ì‚¬ìš©í•  ëª¨ë¸ ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. OpenAI")
        print("2. Azure OpenAI")
        print("3. Anthropic")
        
        try:
            choice = input("ì„ íƒ (1-3, ê¸°ë³¸ê°’: 1): ").strip() or "1"
        except UnicodeDecodeError:
            print("ì…ë ¥ ì¸ì½”ë”© ì˜¤ë¥˜. ìˆ«ìë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            choice = input("Choice (1-3, default: 1): ").strip() or "1"
    
    # ëª¨ë¸ ì„¤ì • ìƒì„±
    if choice == "1":
        # OpenAI ì„¤ì •
        if not os.getenv("OPENAI_API_KEY"):
            print("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   export OPENAI_API_KEY='your-api-key'")
            return
        
        model_config = ModelConfig.from_env(ModelProvider.OPENAI)
        print(f"âœ… OpenAI ëª¨ë¸ ì‚¬ìš©: {model_config.model}")
        
    elif choice == "2":
        # Azure OpenAI ì„¤ì •
        required_vars = ["AZURE_OPENAI_ENDPOINT", "AZURE_OPENAI_DEPLOYMENT"]
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            print(f"âš ï¸  ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
            print("   Azure OpenAI ì‚¬ìš©ì„ ìœ„í•´ ë‹¤ìŒ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            print("   export AZURE_OPENAI_ENDPOINT='https://your-endpoint.openai.azure.com/'")
            print("   export AZURE_OPENAI_DEPLOYMENT='your-deployment-name'")
            print("   export AZURE_OPENAI_API_KEY='your-api-key'  # ë˜ëŠ” Azure AD ì‚¬ìš©")
            return
        
        model_config = ModelConfig.from_env(ModelProvider.AZURE_OPENAI)
        print(f"âœ… Azure OpenAI ëª¨ë¸ ì‚¬ìš©: {model_config.model} (ë°°í¬: {model_config.azure_deployment})")
        
    elif choice == "3":
        # Anthropic ì„¤ì •
        if not os.getenv("ANTHROPIC_API_KEY"):
            print("âš ï¸  ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   export ANTHROPIC_API_KEY='your-api-key'")
            return
        
        model_config = ModelConfig.from_env(ModelProvider.ANTHROPIC)
        print(f"âœ… Anthropic ëª¨ë¸ ì‚¬ìš©: {model_config.model}")
        
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ê°’(OpenAI)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        model_config = ModelConfig.from_env(ModelProvider.OPENAI)
    
    # STORM ì„¤ì • ìƒì„±
    config = StormConfig(
        model_config=model_config,
        max_analysts=args.max_analysts,
        max_interview_turns=args.max_interview_turns,
        parallel_interviews=args.parallel,
        tavily_api_key=os.getenv("TAVILY_API_KEY")
    )
    
    # ì—°êµ¬ ì£¼ì œ ì…ë ¥
    if args.topic:
        topic = args.topic
    else:
        try:
            topic = input("\nğŸ” ì—°êµ¬í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        except UnicodeDecodeError:
            print("ì…ë ¥ ì¸ì½”ë”© ì˜¤ë¥˜. ì˜ë¬¸ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            topic = input("Enter research topic: ").strip()
        if not topic:
            topic = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì™€ ì‚¬íšŒì  ì˜í–¥"
            print(f"ê¸°ë³¸ ì£¼ì œ ì‚¬ìš©: {topic}")
    
    # ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ê²°ì •
    if args.interactive:
        workflow = create_storm_workflow(config, interactive_mode=True)
        mode_name = "ì¸í„°ë™í‹°ë¸Œ"
    elif args.auto:
        workflow = create_storm_workflow(config, interactive_mode=False)
        mode_name = "ì™„ì „ ìë™"
    else:
        # ê¸°ë³¸ê°’: ì‚¬ìš©ìì—ê²Œ ì„ íƒí•˜ê²Œ í•¨
        print("\n" + "="*60)
        print("ğŸ¯ ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
        print("="*60)
        print("1. ğŸ¤ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (ê°„ë‹¨í•œ 2ë‹¨ê³„ í”¼ë“œë°±)")
        print("   - ë¶„ì„ê°€ ìˆ˜ ì„ íƒ")
        print("   - ìµœì¢… ë³´ê³ ì„œ ìŠ¹ì¸")
        print("   - ì‚¬ìš©ìê°€ ì§ì ‘ ì œì–´ ê°€ëŠ¥")
        print("\n2. ğŸš€ ì™„ì „ ìë™ ëª¨ë“œ (ì‚¬ìš©ì ê°œì… ì—†ì´ ìë™ ì‹¤í–‰)")
        print("   - ëª¨ë“  ê³¼ì •ì´ ìë™ìœ¼ë¡œ ì§„í–‰")
        print("   - ë¹ ë¥¸ ê²°ê³¼ ìƒì„±")
        
        while True:
            try:
                mode_choice = input("\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
            except UnicodeDecodeError:
                print("ì…ë ¥ ì¸ì½”ë”© ì˜¤ë¥˜. ìˆ«ìë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                mode_choice = input("Choice (1 or 2): ").strip()
            if mode_choice == "1":
                workflow = create_storm_workflow(config, interactive_mode=True)
                mode_name = "ì¸í„°ë™í‹°ë¸Œ"
                break
            elif mode_choice == "2":
                workflow = create_storm_workflow(config, interactive_mode=False)
                mode_name = "ì™„ì „ ìë™"
                break
            else:
                print("ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (1 ë˜ëŠ” 2)")
    
    # ì—°êµ¬ ì‘ì—… ìƒì„±
    task = ResearchTask(
        topic=topic,
        max_analysts=config.max_analysts,
        max_interview_turns=config.max_interview_turns,
        parallel_interviews=config.parallel_interviews
    )
    
    tracing_manager = get_tracing_manager()
    
    try:
        print(f"\nğŸš€ ì—°êµ¬ ì‹œì‘...")
        print(f"ğŸ¯ ì‹¤í–‰ ëª¨ë“œ: {mode_name}")
        print(f"ğŸ“Š ì„¤ì •: ë¶„ì„ê°€ {task.max_analysts}ëª…, ì¸í„°ë·° í„´ {task.max_interview_turns}íšŒ")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {'í™œì„±í™”' if task.parallel_interviews else 'ë¹„í™œì„±í™”'}")
        print("-" * 60)
        
        result = await workflow.run_research(task)
        
        print("\n" + "=" * 50)
        print("ğŸ“Š ì—°êµ¬ ì™„ë£Œ!")
        print("=" * 50)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ì£¼ì œ: {result.topic}")
        print(f"ğŸ‘¥ ë¶„ì„ê°€ ìˆ˜: {len(result.analysts)}")
        print(f"ğŸ¤ ì¸í„°ë·° ìˆ˜: {len(result.interviews)}")
        
        print("\nğŸ‘¥ ìƒì„±ëœ ë¶„ì„ê°€ë“¤:")
        for i, analyst in enumerate(result.analysts, 1):
            print(f"  {i}. {analyst.name} ({analyst.role}) - {analyst.affiliation}")
        
        # ìµœì¢… ë³´ê³ ì„œ ì €ì¥
        output_file = f"research_report_{topic.replace(' ', '_')[:30]}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result.final_report)
        
        print(f"\nğŸ“„ ìµœì¢… ë³´ê³ ì„œê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # Langfuse ì¶”ì  ì •ë³´ ì¶œë ¥
        if tracing_enabled and tracing_manager.enabled:
            print(f"\nğŸ” Langfuseì—ì„œ ì—°êµ¬ ê³¼ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print(f"   ì¶”ì  ëŒ€ì‹œë³´ë“œ: {os.getenv('LANGFUSE_HOST', 'https://cloud.langfuse.com')}")
        
        # ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“– ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°:")
        print("-" * 30)
        preview = result.final_report[:500] + "..." if len(result.final_report) > 500 else result.final_report
        print(preview)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await workflow.close()
        if tracing_manager.enabled:
            tracing_manager.close()


if __name__ == "__main__":
    asyncio.run(main())
